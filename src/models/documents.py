"""Typed models for source documents and indexable chunks.

Mono-user, chunk-centered design with utilities to convert to/from
LangChain's Document for integration with existing vector stores.
"""

from __future__ import annotations

from datetime import datetime
from hashlib import sha256
from typing import Any, Dict, List, Literal, Optional

from langchain_core.documents import Document as LCDocument
from pydantic import BaseModel, Field


class DocumentUnit(BaseModel):
    """Represents the unit in the source system (Slack issue, page, email, etc.).

    This model describes a single logical document in the origin system. Chunks
    are derived from this unit and reference it via `parent_document_id`.
    """

    document_id: str = Field(description=("Stable internal ID for this source document (generated by our system)."))
    source: str = Field(
        min_length=1,
        description=(
            "Origin of the document as a lowercase slug (e.g., 'slack', 'jira',\n"
            "'confluence', 'email', 'gitlab'). Open set; normalized via validator."
        ),
    )
    source_doc_id: str = Field(description=("Identifier in the source system (e.g., 'JIRA-123', Slack ts, page id)."))
    source_instance_id: Optional[str] = Field(
        default=None,
        description=(
            "Identifier of the specific source instance/account/workspace "
            "(e.g., Slack team/workspace ID) to disambiguate multiple sources of "
            "the same type."
        ),
    )
    uri: Optional[str] = Field(
        default=None,
        description=("Canonical permalink/URL to open the item in the source system."),
    )
    title: Optional[str] = Field(default=None, description="Human-friendly title or subject when available.")
    author: Optional[str] = Field(
        default=None, description="Display name or identifier of the author in the source system."
    )
    parent_id: Optional[str] = Field(
        default=None, description="Reference to a parent DocumentUnit.document_id or source structure."
    )
    language: Optional[str] = Field(default=None, description="BCP-47 language code of the document.")
    source_created_at: Optional[datetime] = Field(
        default=None, description="Creation timestamp in the source system (UTC)."
    )
    source_updated_at: Optional[datetime] = Field(
        default=None, description="Last update timestamp in the source system (UTC)."
    )
    system_tags: List[str] = Field(
        default_factory=list,
        description=("Tags assigned automatically by the system (facets, categories, etc.)."),
    )
    user_tags: List[str] = Field(
        default_factory=list,
        description="Tags added manually by the user for organization/filtering.",
    )
    status: Literal["active", "archived", "deleted"] = Field(
        default="active",
        description=("Lifecycle state of the document in our store (not the source)."),
    )
    source_metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description=("Source-specific structured attributes (e.g., channel, project, space)."),
    )
    content_hash: Optional[str] = Field(
        default=None,
        description=("SHA-256 of the raw source content used for dedup/version detection."),
    )

    @staticmethod
    def compute_hash(raw: bytes) -> str:
        """Compute SHA-256 hash of raw bytes."""
        return sha256(raw).hexdigest()


class ChunkRecord(BaseModel):
    """Represents the indexable unit (chunk) derived from a source document.

    Stores content, positional offsets, embeddings metadata, and tags used for
    retrieval and re-localization in the original source.
    """

    chunk_id: str = Field(description="Stable internal ID for this chunk (unique within our store).")
    parent_document_id: str = Field(description="Reference to the parent DocumentUnit.document_id.")
    parent_chunk_id: Optional[str] = Field(
        default=None,
        description="Optional reference to the parent chunk_id for hierarchical chunking.",
    )
    level: Optional[int] = Field(
        default=None,
        description="Optional depth level in a hierarchical structure (0 for top-level).",
    )
    chunk_index: int = Field(description="Zero-based index of the chunk within its parent document.")
    text: str = Field(description="Plain text content of the chunk.")
    language: Optional[str] = Field(default=None, description="BCP-47 language code when known (e.g., 'es', 'en').")
    mime_type: Optional[str] = Field(
        default=None,
        description="MIME type of the original content (e.g., 'text/plain').",
    )
    page: Optional[int] = Field(
        default=None,
        description="Page number for paged sources (PDF, docs), if applicable.",
    )
    start_line: Optional[int] = Field(
        default=None,
        description="Start line offset in the original content, if applicable.",
    )
    end_line: Optional[int] = Field(
        default=None,
        description="End line offset in the original content, if applicable.",
    )
    start_char: Optional[int] = Field(default=None, description="Start character offset within the source text.")
    end_char: Optional[int] = Field(default=None, description="End character offset within the source text.")
    retrieved_at: datetime = Field(
        default_factory=datetime.utcnow,
        description="Timestamp when this chunk was extracted (UTC).",
    )
    indexed_at: Optional[datetime] = Field(
        default=None,
        description="Timestamp when this chunk was embedded/indexed (UTC).",
    )
    embedding_model: str = Field(
        default="sentence-transformers/all-MiniLM-L6-v2",
        description="Embedding model identifier used to compute vectors.",
    )
    embedding_version: Optional[str] = Field(
        default=None, description="Embedding model version/commit/tag, if available."
    )
    vector_dim: Optional[int] = Field(default=None, description="Dimension of the embedding vector for this chunk.")
    token_count: Optional[int] = Field(default=None, description="Token count of the chunk text (model-dependent).")
    vector_id: Optional[str] = Field(
        default=None,
        description=("Identifier/primary key of the vector in the vector store (when applicable)."),
    )
    system_tags: List[str] = Field(
        default_factory=list,
        description="System-assigned tags for filtering/ranking (auto-generated).",
    )
    user_tags: List[str] = Field(default_factory=list, description="User-assigned tags for organization.")
    content_hash: Optional[str] = Field(
        default=None,
        description=("SHA-256 of the chunk text/content for deduplication and change detection."),
    )
    chunking_strategy: Optional[str] = Field(default=None, description="Name/identifier of the chunking strategy used.")
    chunk_overlap: Optional[int] = Field(
        default=None,
        description="Number of overlapping characters/tokens between chunks.",
    )

    def to_langchain_document(self, doc: DocumentUnit) -> LCDocument:
        """Convert this chunk plus its parent document into a LangChain Document."""
        metadata: Dict[str, Any] = {
            "source": doc.source,
            "source_instance_id": doc.source_instance_id,
            "source_doc_id": doc.source_doc_id,
            "uri": doc.uri,
            "title": doc.title,
            "author": doc.author,
            "parent_id": doc.parent_id,
            "doc_language": doc.language,
            "parent_document_id": self.parent_document_id,
            "parent_chunk_id": self.parent_chunk_id,
            "document_id": doc.document_id,
            "chunk_id": self.chunk_id,
            "chunk_index": self.chunk_index,
            "level": self.level,
            "page": self.page,
            "start_line": self.start_line,
            "end_line": self.end_line,
            "start_char": self.start_char,
            "end_char": self.end_char,
            "retrieved_at": self.retrieved_at.isoformat(),
            "indexed_at": self.indexed_at.isoformat() if self.indexed_at else None,
            "embedding_model": self.embedding_model,
            "embedding_version": self.embedding_version,
            "vector_dim": self.vector_dim,
            "token_count": self.token_count,
            "vector_id": self.vector_id,
            "system_tags": sorted(set(doc.system_tags + self.system_tags)),
            "user_tags": sorted(set(doc.user_tags + self.user_tags)),
            "language": self.language,
            "mime_type": self.mime_type,
            "status": doc.status,
            **{f"src_{k}": v for k, v in doc.source_metadata.items()},
        }
        return LCDocument(page_content=self.text, metadata=metadata)

    @staticmethod
    def from_langchain_document(lc_doc: LCDocument) -> "ChunkRecord":
        """Reconstruct a ChunkRecord from a LangChain Document's content/metadata."""
        m = lc_doc.metadata
        return ChunkRecord(
            chunk_id=str(m.get("chunk_id")),
            parent_document_id=str(m.get("parent_document_id")),
            parent_chunk_id=m.get("parent_chunk_id"),
            chunk_index=int(m.get("chunk_index", 0)),
            level=int(m["level"]) if m.get("level") is not None else None,
            text=lc_doc.page_content,
            language=m.get("language"),
            mime_type=m.get("mime_type"),
            page=m.get("page"),
            start_line=m.get("start_line"),
            end_line=m.get("end_line"),
            start_char=m.get("start_char"),
            end_char=m.get("end_char"),
            retrieved_at=(datetime.fromisoformat(m["retrieved_at"]) if m.get("retrieved_at") else datetime.utcnow()),
            indexed_at=(datetime.fromisoformat(m["indexed_at"]) if m.get("indexed_at") else None),
            embedding_model=str(m.get("embedding_model", "")),
            embedding_version=m.get("embedding_version"),
            vector_dim=m.get("vector_dim"),
            token_count=m.get("token_count"),
            vector_id=m.get("vector_id"),
            system_tags=list(m.get("system_tags", [])),
            user_tags=list(m.get("user_tags", [])),
            chunking_strategy=m.get("chunking_strategy"),
            chunk_overlap=m.get("chunk_overlap"),
            content_hash=m.get("content_hash"),
        )
